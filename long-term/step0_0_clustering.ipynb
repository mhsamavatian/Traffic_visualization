{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import math\n",
    "from IPython.display import display\n",
    "from multiprocessing import cpu_count,Pool \n",
    "from joblib import Parallel, delayed\n",
    "import psutil\n",
    "from sklearn.cluster import KMeans, SpectralClustering, DBSCAN \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from itertools import cycle, islice\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint\n",
    "import gmaps\n",
    "#import hdbscan\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_usage():\n",
    "    print (\"memory log:\")\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(\"%5.2f GB (RSS)\" % (process.memory_info().rss / 2**30))\n",
    "    print(\"%5.2f GB (VMS)\" % (process.memory_info().vms / 2**30))\n",
    "    print(\"%5.2f GB (Used)\" % (psutil.virtual_memory().used / 2**30))\n",
    "    print(\"%5.2f GB (Available)\" % (psutil.virtual_memory().available / 2**30))\n",
    "    print(\"%5.2f GB (Total)\" % (psutil.virtual_memory().total / 2**30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of cores 28\n"
     ]
    }
   ],
   "source": [
    "cores = cpu_count() #Number of CPU cores on your system\n",
    "partitions = cores #Define as many partitions as you want\n",
    "print (\"number of cores \"+str(partitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data set\n",
      "Data set size:  15325090\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "print (\"load data set\")\n",
    "data_set = pd.read_hdf('../../data_set.h5',key='DS_new')\n",
    "print (\"Data set size: \", data_set.shape[0])\n",
    "#display(data_set)\n",
    "print (\"*\"*80)\n",
    "# filter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13147811\n",
      "after sampling  2000000\n"
     ]
    }
   ],
   "source": [
    "traffic_set = data_set[data_set.Type == 'T']\n",
    "print (traffic_set.shape[0])\n",
    "traffic_set = traffic_set.sample(n=2000000,replace=True)\n",
    "print ('after sampling ',traffic_set.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda5/lib/python3.6/site-packages/pandas/core/generic.py:1299: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->['EventId', 'Type', 'RefinedType_', 'AirportCode', 'Street', 'Side', 'City', 'County', 'State', 'RefinedType']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "traffic_set.to_hdf('../../data_set.h5',key='DS_new_sampled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get traffic data and select whole or subset of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 records\n"
     ]
    }
   ],
   "source": [
    "coords = traffic_set.as_matrix(columns=['LocationLat', 'LocationLng'])\n",
    "print ('Loaded {} records'.format(len(coords)))\n",
    "data = np.radians(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kms_per_radian = 3959\n",
    "epsilon = 4.09/ kms_per_radian #default: 4.09 km as neighborhood threshold!\n",
    "clusterer = DBSCAN(eps=epsilon, min_samples=10, algorithm='auto', metric='haversine',n_jobs=cores).fit(data)\n",
    "cluster_labels = clusterer.labels_\n",
    "num_clusters = len(set(cluster_labels))\n",
    "clusters = pd.Series([coords[cluster_labels == n] for n in range(num_clusters)])\n",
    "print('Number of clusters: {}'.format(num_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set parameters and make clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kms_per_radian = 3959\n",
    "epsilon = 4.09 / kms_per_radian #default: 1.5 km as neighborhood threshold!\n",
    "clusterer = DBSCAN(eps=epsilon, min_samples=463, algorithm='auto', metric='haversine',n_jobs=cores).fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store the result of clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../clustering_cordinates\",data)\n",
    "loaded_data = np.load(\"../../clustering_cordinates\"+\".npy\")\n",
    "np.save(\"../../cluster_labels\",clusterer.labels_)\n",
    "loaded_labels = np.load(\"../../cluster_labels\"+\".npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 [python/3.6]",
   "language": "python",
   "name": "sys_python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
